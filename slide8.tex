%\documentclass{beamer}
\documentclass[handout]{beamer}
% \usepackage{beamerthemesplit} // Activate for custom appearance
\usetheme{Boadilla}
\newtheorem{Proposition}[theorem]{Proposition}%
\newcommand{\BP}{\mathbf{P}}
\newcommand{\BE}{\mathbf{E}}
\newcommand{\BI}{\mathbf{1}}


\setbeamertemplate{theorems}[numbered]

\title{STAT 7200}
 \subtitle{Introduction to Advanced Probability \newline Lecture 8}
\author{Taylor R. Brown}
\institute{}
\date{}

\begin{document}

\frame{\titlepage}

\section[Outline]{}
\frame{\tableofcontents
``A First Look at Rigorous Probability Theory" (Jeffrey Rosenthal) Sections 3.3, 3.4, and 4.1}



\section{Probability Triple}
\subsubsection{Uniqueness of Extension Theorem}

\frame
{
  \frametitle{Uniqueness of Extension Theorem: continued}

   \begin{itemize}
     
    
            
                   
\item<1->[]    
\begin{Corollary} 
Let $\mathcal{J}$ be a semi-algebra and $\mathcal{F}$ be the $\sigma-algebra$ generated by  $\mathcal{J}$. Let $\mathbf{P}$ and $\mathbf{Q}$ be two probability measures over $\mathcal{F}$, so that $\mathbf{P}(A)=\mathbf{Q} (A)$ for any $A\in\mathcal{J}$. Then $\mathbf{P}(A)=\mathbf{Q} (A)$ for any $A\in\mathcal{F}$. 
\end{Corollary}    
                 
\item<2->[]    
\begin{Corollary} Let $\mathbf{P}$ and $\mathbf{Q}$ be two probability measures over $\mathcal{B}$,  the collection of Borel sets, so that $\mathbf{P}((-\infty, x])=\mathbf{Q} ((\infty,x])$ for any $x\in \mathbf{R}$. Then $\mathbf{P}(A)=\mathbf{Q} (A)$ for any $A\in\mathcal{B}$. 
\end{Corollary}    
\end{itemize}

}


\section{Foundations of Probability}

\subsection{Random Variable}

\subsection{Independence}


\subsubsection{Independence and Uniqueness of Measure}
\frame
{
  \frametitle{Independence and Uniqueness of Measure}

   \begin{itemize}

\item<1->[]  
\begin{Proposition}
Let $X$ and $Y$ be random variables jointly defined on the probability triple $(\Omega, \mathcal{F}, \BP)$. Then $X$ and $Y$ are independent if and only if $\BP(X\leq x, Y\leq y)=\BP(X\leq x)\BP (Y\leq y)$ for all the real numbers $x$ and $y$.
\end{Proposition}
             
\item<2->  
\textbf{proof} The ``only if" part is trivial. To prove the ``if" part, we need to show $\BP(X\leq x, Y\leq y)=\BP(X\leq x)\BP (Y\leq y)$ implies $\BP(X\in A, Y\in B)=\BP(X\in A)\BP (Y\in B)$ for all Borel sets $A$ and $B$, which requires the uniqueness of extension theorem. 

\item<3->[-] 
Define $\mathbf{Q}_x(S)=\BP(X\leq x, Y\in S)/\BP(X\leq x)$ for fixed $x$ with $\BP(X\leq x)>0$. We can verify $\mathbf{Q}_x$ is a proper probability measure over $\mathcal{B}$. Furthermore, since $\BP(X\leq x, Y\leq y)=\BP(X\leq x)\BP (Y\leq y)$, we have $\mathbf{Q}_x((-\infty, y])=\mathbf{P}(Y\leq y)$. Then the uniqueness of the extension theorem ensures that $\mathbf{Q}_x(B)=\BP(Y\in B)$ for all Borel set $B$, which further implies $\BP(X\leq x, Y\in B)=\BP(X\leq x)\BP( Y\in B)$. 
                           
\end{itemize}
}


\frame
{
  \frametitle{Independence and Uniqueness of Measure: continued}

   \begin{itemize}

\item<1->  
\textbf{Proof: continued}: As  $\BP(X\leq x, Y\in B)=\BP(X\leq x)\BP( Y\in B)$ for any $x$ and Borel set $B$. Let us define another function $\mathbf{R}_B (X\in S)=\BP(X\in S, Y\in B)/\BP(Y\in B)$ for fixed $B$. Again, $\mathbf{R}_B$ is a proper probability measure. 

\item<2->  We can also verify that $\mathbf{R}_B (X\leq x)=\BP(X\leq x)$. Then by the uniqueness of extension theorem again, $\mathbf{R}_B(X \in (-\infty,x]) = P(X \le x)$ guarantees $\BP(X\in S, Y\in B)=\mathbf{R}_B (X\in S)\BP(Y\in B)=\BP(X\in S)\BP(Y\in B)$.

\end{itemize}
}

\subsection{Limit of Events}

\subsubsection{Continuity of Probability}


\frame
{
  \frametitle{Continuity of Probability (Proposition 3.3.1)}

   \begin{itemize}

\item<1->  \textbf{Limit of Increasing and Decreasing Events}: The limit of an increasing sequence of events $A_1, A_2,\cdots$ where $A_1\subseteq A_2\subseteq \cdots$ is defined as the union: $\lim_{n\rightarrow \infty} A_n=\bigcup_n A_n$.  For a decreasing sequence $A_1, A_2,\ldots$ where $A_1\supseteq A_2\supseteq \cdots$, the limit is defined as the intersection: $\lim_{n\rightarrow \infty} A_n=\bigcap_n A_n$. 
                       
\item<2->  \textbf{Continuity of Probabilities} *In the above scenarios*, by countable additivity, we have $\lim_{n\rightarrow \infty}\BP(A_n)=\BP(A)$.  (See  of the textbook).                                       

\item<3->  If $\{A_n\} \nearrow A$, define $B_k = A_k \cap A_{k-1}^c$ :
\begin{align*}
&\BP(\lim_n A_n) = \BP(\cup_{k\ge1} A_k) = \BP(\cup_{k\ge1} B_k) = \sum_{k = 1}^\infty \BP(B_k) \\
&= \lim_{n \to \infty} \sum_{k = 1}^n \BP(B_k) = \lim_{n \to \infty} \BP( \cup_{ k =1}^n B_k) =\lim_{n \to \infty} \BP(A_n)
\end{align*}

                  
                 \end{itemize}
}

\frame
{
  \frametitle{Continuity of Probability}

   \begin{itemize}

\item<1->  If $\{A_n\} \searrow A := \lim_{n} A_n$, then $\{A_n^c\} \nearrow A^c := \lim_{n \to \infty}A_n^c$ and $\lim_{n \to \infty}\BP(A_n^c) = \BP(\lim_{n \to \infty}A_n^c)$ and
\begin{align*}
&\BP(\lim_n A_n) = \BP(\cap_n A_n) = 1- \BP(\cup_n A_n^c) = 1 - \lim_{n \to \infty}\BP(A_n^c) \\
&= \lim_{n \to \infty}\left[1 - \BP(A_n^c)\right] = \lim_n \BP( A_n)
\end{align*}

\item<2->  Just as is true for sequences of real numbers, not all sequences of events have a limit. E.g. on $([0,1],\mathcal{M},\BP)$ consider $A_n = [0,\frac{3}{4})$ if $n$ is even, and $[\frac{1}{3},1]$ if $n$ is odd. 

\item<3->  Recall, for real-valued sequences:
               
$\liminf_{n\rightarrow \infty} x_n=\lim_{n\rightarrow \infty} (\inf_{m\geq n} x_m)$  $\limsup_{n\rightarrow \infty} x_n=\lim_{n\rightarrow \infty} (\sup_{m\geq n} x_m)$.\\
               
\item<4->  We'll apply the same idea to events.

\end{itemize}
}

\subsubsection{Limits of Events}


\frame
{
  \frametitle{Limits of Events}

   \begin{itemize}


    \item<1->  
      \textbf{Limit Infimum and Limit Supremum of Events}: For *any* sequence of events $A_1, A_2,\ldots$ define:
$$\limsup_{n\rightarrow \infty} A_n=\bigcap_{n=1}^{\infty} \bigcup_{k=n}^{\infty} A_k;$$
$$\liminf_{n\rightarrow \infty} A_n=\bigcup_{n=1}^{\infty} \bigcap_{k=n}^{\infty} A_k$$  

\item<2->  $C_n := \bigcup_{k=n}^{\infty} A_k$ is decreasing, so its limit is an intersection.
               
\item<3->  $D_n := \bigcap_{k=n}^{\infty} A_k$ is increasing, so its limit is a union.

                 \end{itemize}
}

\frame
{
  \frametitle{Limits of Events}

   \begin{itemize}


    \item<1->  
$$\limsup_{n\rightarrow \infty} A_n = \bigcap_{n=1}^{\infty} \bigcup_{k=n}^{\infty} A_k;$$
$$\liminf_{n\rightarrow \infty} A_n=\bigcup_{n=1}^{\infty} \bigcap_{k=n}^{\infty} A_k$$  

\item<2->  $\left[ \limsup_{n\rightarrow \infty} A_n \right]^c = \liminf_{n\rightarrow \infty} A_n^c$

\item<3-> $ \liminf_n A_n \subseteq \limsup_n A_n$.


                 \end{itemize}
}

\frame
{
  \frametitle{Limits of Events}

   \begin{itemize}


    \item<1->  
      \textbf{Limit Infimum and Limit Supremum of Events}: For *any* sequence of events $A_1, A_2,\ldots$ define:
$$\limsup_{n\rightarrow \infty} A_n=\bigcap_{n=1}^{\infty} \bigcup_{k=n}^{\infty} A_k;$$
$$\liminf_{n\rightarrow \infty} A_n=\bigcup_{n=1}^{\infty} \bigcap_{k=n}^{\infty} A_k$$  

\item<2->  The event $\limsup_{n\rightarrow \infty} A_n$ occurs if for any $n\in \mathbf{N}$, there is at least one $k \geq n$ such that $A_k$ occurs. Thus, this event is also referred as ``$A_n$ infinitely often", or ``$A_n$ i.o.". 

\item<3->  The event $\liminf_{n\rightarrow \infty} A_n$ occurs if there is a $n\in \mathbf{N}$, and all the event $A_k$ with $k\geq n$ occur. Thus, this event is also referred as ``$A_n$ almost always", or ``$A_n$ a.a.".
               

                 \end{itemize}
}

\frame
{
  \frametitle{Limits of Events}
:

   \begin{itemize}
  
      \item<1->  Consider $([0,1],\mathcal{B},\BP)$ and define $A_n = [0,\frac{3}{4})$ if $n$ is even, and $[\frac{1}{3},1]$ if $n$ is odd. What is $\{A_n \text{ i.o.}\}$? What is $\{A_n \text{ a.a}\}$?
      
    \item<2->  Consider $([0,1],\mathcal{B},\BP)$ and define $A_n = [0,1/2)$ if $n$ is even, and $[1/2,1]$ if $n$ is odd. What is $\{A_n \text{ i.o.}\}$? What is $\{A_n \text{ a.a}\}$?

    \item<3->  Consider $(\Omega, \mathcal{F}, \BP)$ for the infinite fair coin tossing experiment and define $H_n$ is the event that the $n$th coin-toss is a heads. What is $\{A_n \text{ i.o.}\}$? What is $\{A_n \text{ a.a.}\}$?

  \end{itemize}
}


\frame
{
  \frametitle{Limits of Events}

   \begin{itemize}


\item<1->[] 
\begin{Proposition}[3.4.1] 
$$\BP(\liminf_{n\rightarrow \infty} A_n) \leq \liminf_{n\rightarrow \infty} \BP(A_n)\leq \limsup_{n\rightarrow \infty} \BP(A_n)\leq \BP(\limsup_{n\rightarrow \infty} A_n) $$
\end{Proposition}
                           
\item<2->  \textbf{Proof:} By the continuity of probability, 
                             
$\BP(\limsup_{n\rightarrow \infty} A_n)=\BP(\bigcap_{n=1}^{\infty} \bigcup_{k=n}^{\infty} A_k)=\lim_{n\rightarrow \infty} \BP(\bigcup_{k=n}^{\infty} A_k) = \limsup_{n\rightarrow \infty} \BP(\bigcup_{k=n}^{\infty} A_k) \geq  \limsup_{n\rightarrow \infty} \BP(A_n )$
               
\item<2-> [-] Similarly, we may prove  $\BP(\liminf_{n\rightarrow \infty} A_n) \leq \liminf_{n\rightarrow \infty} \BP(A_n)$. And  $\liminf_{n\rightarrow \infty} \BP(A_n)\leq \limsup_{n\rightarrow \infty} \BP(A_n)$ by the definition of the limit infimum and limit supremum of real numbers.                  \end{itemize}
}

\subsection{Borel-Cantelli Lemma}

\frame
{
  \frametitle{Borel-Cantelli Lemma}

   \begin{itemize}


\item<1->[] \begin{Theorem} For $A_1,A_2,\ldots \in \mathcal{F}$, 
                       
(i) If $\sum_n \BP(A_n)<\infty$, then $\BP(\limsup_n A_n)=0$. 
                      
(ii) If $\sum_n \BP(A_n)=\infty$, $\{A_n\}$ are independent, then $\BP(\limsup_n A_n)=1$. 
                      

\end{Theorem}
                           
               \item<2->  \textbf{Proof:} For (i), 
        $\BP(\limsup_n A_n)= \BP(\bigcap_{m=1}^{\infty}\bigcup_{k=m}^{\infty} A_k) \leq \BP(\bigcup_{k=m}^{\infty} A_k) \leq \sum_{k=m}^{\infty} \BP(A_k)$,
        
       which converges to $0$ as $m\rightarrow \infty$ if $\sum_n \BP(A_n)<\infty$.       
       
       \item<3->[] For (ii), we need to show  $\BP((\limsup_n A_n)^c)=\BP(\bigcup_{n=1}^{\infty} \bigcap_{k=n}^{\infty} A_k^C)=0$. It would be sufficient if we can show $\BP( \bigcap_{k=n}^{\infty} A_k^C)=0$ for any $n\in \mathbf{N}$:
       
      \item<4->[] By independence,  $\BP( \bigcap_{k=n}^{\infty} A_k^C)\leq \BP( \bigcap_{k=n}^{m} A_k^C)=\prod_{k=n}^m (1-\BP(A_k))$
      
            \item<5->[] Since for any real number, $1-x\leq e^{-x}$, and $\sum_{k=n}^{\infty} \BP(A_k)=\infty$:
            
            $\prod_{k=n}^m (1-\BP(A_k))\leq \prod_{k=n}^m e^{-\BP(A_k)}=e^{-\sum_{k=n}^m \BP(A_k)}\rightarrow 0$  as $m\rightarrow \infty$.
       
       
            \item<6->[] Thus $\BP( \bigcap_{k=n}^{\infty} A_k^C)=0$ for any $n\in \mathbf{N}$, and $\BP(\limsup_n A_n)=1$.
                
                             
                    \end{itemize}
}

\frame
{
  \frametitle{Borel-Cantelli Lemma: Example}

   \begin{itemize}


\item<1-> Going back to the infinite coin tossing example, let $H_n$ be the event that the n$th$ toss comes up ``Heads." Assume that $\BP(H_n)=p>0$. Then, as these events are independent, and as $\sum_n \BP(H_n)=\infty$, $\BP(\limsup_n H_n)=1$. That is, if you toss a coin infinite times, if the chance of obtaining heads is not zero, then with probability $1$, an infinite sequence of tosses would contain infinite number of heads. 
                                                  
\item<2->  Another example: define $B_n : H_n \cap \cdots \cap H_{n+999}$.  Although $\sum_n \BP(B_n)=\infty$, the Borel-Cantelli lemma is not *directly* applicable as $\{B_n\}$ are not independent. 

\item<3->  However, we can focus on a subsequence: $\{B_{k_n}\}$, where $k_n=1+(n-1)*1000$. $\{B_{k_n}\}$ would be a sequence of independent events and we can use Borel-Cantelli lemma to show that $B_{k_n}$ occurs infinitely often, so $B_n$ must also occur infinitely often as well.
               
\item<3-> This lemma will be very useful when we discuss the relationship between convergence almost surely and convergence in probability.                 

\end{itemize}
}

\subsection{Expectations}


\frame
{
  \frametitle{Expectations}

   \begin{itemize}


\item<1-> You are already familiar with the concept of expectation from introductory classes. In the next lecture, we will develop the rigorous definition of expected value! 
                       
\item<2-> Next lecture's roadmap: 
\begin{itemize}
\item define it for *simple* random variables;
\item define it for nonnegative random variables;
\item define it for general random variables.
\end{itemize}

In this process, we will show that the expectation (still) has all the properties that you know and love, such as linearity and the fact that it is order preserving. 

\end{itemize}
}

\frame
{
  \frametitle{Expectations and Integrals}

   \begin{itemize}

\item<1-> One key idea behind the rigorous definition of an expectation is:  when we calculate the average of a random variable $X$, rather than summing up (or integrating) the $X(\omega)$ for all $\omega\in \Omega$, we partition the sample space based on the values of $X(\omega)$. Outcomes with similar values of $X(\omega)$ will be grouped together, and we calculate the average of $X(\omega)$ based on such partitions. 

\item<2-> This idea is fundamentally different from standard Riemann integration described in a calculus class. 
                         
\item<3-> Here is an illustration: suppose I have a pile of bills with values: \$1, \$5, \$1, \$5, \$100. To calculate the total amount of money in this pile, I can either 
\begin{enumerate}
\item proceed in whatever order it's in:  calculate \$1+ \$ 5 + \$1 + \$5 + \$100 (i.e. Riemann)
\item organize the bills by value: 2  \$1 bills, 2 \$ 5 bills, 1 \$ 100 bills (i.e. Lebesgue)
\end{enumerate}

\end{itemize}
}


\end{document}
