%\documentclass{beamer}
\documentclass[handout]{beamer}
% \usepackage{beamerthemesplit} // Activate for custom appearance
\usetheme{Boadilla}
\newtheorem{Proposition}[theorem]{Proposition}%
\newcommand{\BP}{\mathbf{P}}

\setbeamertemplate{theorems}[numbered]

\title{STAT 7200}
 \subtitle{Introduction to Advanced Probability \newline Lecture 7}
\author{Taylor R. Brown}
\institute{}
\date{}

\begin{document}

\frame{\titlepage}

\section[Outline]{}
\frame{\tableofcontents
``A First Look at Rigorous Probability Theory" (Jeffrey Rosenthal) Sections 2.5 (continued), 2.6, 3.1, and 3.2}


\section{Probability Triple}

\subsubsection{Extension Theorem}
\frame
{
  \frametitle{Extension Theorem}

   \begin{itemize}

\item<1-> [] \begin{Theorem} \textbf{The Extension Theorem}  Let $\mathcal{J}$ be a semialgebra of subsets of $\Omega$,  $\mathbf{P}$ a function from $\mathcal{J}$  to [0,1] with the following properties:
             \newline
             
a) $\mathbf{P} (\emptyset)=0, \mathbf{P}(\Omega)=1$.
                 \newline
         
b) $\BP(\bigcup_{i=1}^k A_i)\geq \sum_{i=1}^k \BP(A_i)$ whenever $A_1,\ldots,A_k \in\mathcal{J}$, $\bigcup_{i=1}^k A_i  \in\mathcal{J}$, and $A_1,\ldots,A_k$ are pairwise disjoint (\textit{finite superadditivity}).
                   \newline
       
c) $\BP(A)\leq \sum_{n} \BP(A_n)$ whenever $A,A_1,A_2,\ldots \in\mathcal{J}$, and $A\subseteq \bigcup_{n} A_n$ (\textit{countable monotonicity}).               
             \newline
             
Then there is a $\sigma$-algebra $\mathcal{M} \supseteq \mathcal{J}$ and a proper probability measure $\BP^*$ on $\mathcal{M}$ such that $\BP^*(A)=\BP(A)$ for all $A\in \mathcal{J}$.

\end{Theorem}    
       
                
\end{itemize}
}




\subsubsection{Application of Extension Theorem: Uniform Measure on [0,1]}



\subsubsection{Variation of Extension Theorem}
\frame
{
  \frametitle{Variation of Extension Theorem}

   \begin{itemize}

            
         \item<1->[]    \begin{Proposition} In the original extension theorem, the finite superadditivity condition and the countable monotonicity condition of $\mathbf{P}$ can be replaced by the following countable additivity condition:
            
                    $\BP(\bigcup_{n} A_n)= \sum_{n} \BP(A_n)$ for disjoint $A_1,A_2,\ldots \in\mathcal{J}$ with $\bigcup_{n} A_n  \in\mathcal{J}$.
            
            \end{Proposition}    
       
                                           

                   \end{itemize}
}






\subsubsection{Uniqueness of Extension Theorem}
\frame
{
  \frametitle{Uniqueness of Extension Theorem}

   \begin{itemize}
       
       \item<1->[]    \begin{Theorem}[Proposition 2.5.7] \textbf{Uniqueness of Extension }In the extension theorem (or variation), the extended probability measure $\BP^*$ over $\mathcal{M}$ is unique in the sense that:
       
       For $\sigma-$algebra $\mathcal{F}$ such that $\mathcal{J} \subseteq \mathcal{F} \subseteq \mathcal{M}$ and another probability measure $\mathbf{Q}$ over $\mathcal{F}$ such that  $\mathbf{Q}(A)=\BP(A)$ for all $A\in\mathcal{J}$. Then  $\mathbf{Q}(A)=\BP^*(A)$ for all $A\in\mathcal{F}$.
            
            
\end{Theorem}    
    
\item<2->\textbf{Proof:} For any $A\in\mathcal{F}$
            
\begin{align*} \BP^*(A)& =\inf_{A_1,A_2,\ldots \in \mathcal{J}, A\subseteq \bigcup_i A_i} \sum_i \BP(A_i) =\inf_{A_1,A_2,\ldots \in \mathcal{J}, A\subseteq \bigcup_i A_i} \sum_i \mathbf{Q}(A_i) \\ 
&\geq \inf_{A_1,A_2,\ldots \in \mathcal{J}, A\subseteq \bigcup_i A_i} \mathbf{Q}\left( \bigcup_i A_i\right)   (\text{ countable subadditivity }) \\
&\geq \inf_{A_1,A_2\ldots, \in \mathcal{J}, A\subseteq \bigcup_i A_i} \mathbf{Q}(A)   (\text{ by monotonicity }) = \mathbf{Q}(A). 
\end{align*}

            
                   \end{itemize}
}


\frame
{
  \frametitle{Uniqueness of Extension Theorem: continued}

   \begin{itemize}
     
    
            \item<1->\textbf{Proof (continued):} The previous derivation shows that $ \BP^*(A)\geq \mathbf{Q} (A)$ for any $A\in\mathcal{F}$. Similarly, $ \BP^*(A^c)\geq \mathbf{Q} (A^c)$. But as the probability of complement equals 1 minus the probability, we have $ \BP^*(A)\leq \mathbf{Q} (A)$, thus  $ \BP^*(A)= \mathbf{Q} (A)$. The extension is unique over $\mathcal{F}.$
            
                   
    \item<2->[]    \begin{Corollary}[Proposition 2.5.8] Let $\mathcal{J}$ be a semi-algebra and $\mathcal{F}$ be the $\sigma-algebra$ generated by  $\mathcal{J}$. Let $\mathbf{P}$ and $\mathbf{Q}$ be two probability measures over $\mathcal{F}$, such that $\mathbf{P}(A)=\mathbf{Q} (A)$ for any $A\in\mathcal{J}$. Then $\mathbf{P}(A)=\mathbf{Q} (A)$ for any $A\in\mathcal{F}$. 
                
            \end{Corollary}    
                 
                     \item<3->[]    \begin{Corollary}[2.5.9] Let $\mathbf{P}$ and $\mathbf{Q}$ be two probability measures over $\mathcal{B}$,  the collection of Borel sets, such that $\mathbf{P}((-\infty, x])=\mathbf{Q} ((\infty,x])$ for any $x\in \mathbf{R}$. Then $\mathbf{P}(A)=\mathbf{Q} (A)$ for any $A\in\mathcal{B}$. 
                
            \end{Corollary}    
                   \end{itemize}

}


\subsection{Extension Theorem: Application}

\subsubsection{Extension Theorem: Tossing an Infinite Number of Coins}

\frame
{
  \frametitle{Tossing an Infinite Number of Coins}

   \begin{itemize}
        
            \item<1-> The sample space of tossing a fair coin an infinite number of times can be denoted as: $\Omega=\{(r_1, r_2, r_3,\ldots): r_i=0 \text{ or } 1\}$.            
                   
    \item<2-> Each outcome in this sample space consists of an infinite number of tosses, and each toss equals 0 or 1 with probability 0.5. Then intuitively the probability of each outcome should be 0. However, just as ``the probability of $X=x$ should equal 0 if $X\sim Unif$", this result does not help us much in understanding this particular sample space. 
        
 \item<3->  Denote $A_{a_1a_2\ldots a_n}$ ($a_i= 0 \text{ or } 1 $) as the event that the results of the first $n$ tosses are exactly $a_1,a_2, \ldots, a_n$, then the collection $\mathcal{J}=\{A_{a_1a_2\ldots a_n}: n\in \mathbf{N}, a_i= 0 \text{ or } 1 \} \bigcup \{\emptyset, \Omega\}$ is a semi-algebra. The probability function $\BP$ over $\mathcal{J}$ can be defined as $\BP(A_{a_1a_2\ldots a_n})=1/2^n$. And we can verify that $\BP$ satisfies the variation of extension theorem.
          \item<5-> By the extension theorem, we may extend both $\mathcal{J}$  and  $\BP$ to a proper probability triple. 
                       \item<6-> This probability triple is actually equivalent to the uniform measure (Lebesgue Measure) as each $x\in [0,1]$ can be represented as: $x=\sum_{k=1}^{\infty} \frac{a_k}{2^k}$ in a binary representation.  

                   \end{itemize}

}


\subsubsection{Extension Theorem: Product Measure}

\frame
{
  \frametitle{Product Measure}

   \begin{itemize}
        
            \item<1-> The extension theorem is not limited to one-dimensional sample spaces. We just used it on infinite-dimensional coin-flip spaces, and we can also use it to define a uniform measure over $[0,1]\times [0,1]$.
            
            \item<2->We may construct the semi-algebra as the collection of all the rectangles (may be closed or open on any of the four borders), and define $\BP$ as the area of any rectangle. We then verify the conditions of the extension theorem as we had done for the uniform measure over [0,1] and apply the extension theorem to construct a probability triple. 

    \end{itemize}          
}


\frame{
\frametitle{Product Measure: continued}
       \begin{itemize}
                     \item<1-> Suppose we have two probability measures $(\Omega_1, \mathcal{F}_1, \BP_1)$ and $(\Omega_2, \mathcal{F}_2, \BP_2)$. To define a probability measure over $\Omega_1\times\Omega_2$, we may choose $\mathcal{J}$ as:
                     
                     $$\mathcal{J}=\{A\times B: A\in \mathcal{F}_1, B\in \mathcal{F}_2\}$$
 and define $\BP(A\times B)=\BP(A)\times \BP(B)$
        . 
                   
    \item<2-> It is quite easy to verify that  $\mathcal{J}$ is a semi-algebra.
    % (it is not a $\sigma$-algebra even both $\mathcal{F}_1, \mathcal{F}_2$ are $\sigma$-algebra, for instance, if $\Omega_1=\Omega_2=[0,1]$ and $\mathcal{F}_1, \mathcal{F}_2$ are Borel measurable set, the set $\{(x_1,x_2): x_1+x_2\leq 0.5\}$ is not included in $\mathcal{J}$). 
    
        \item<3-> We will show that $\BP(A\times B)$ is countably additive later. Then we may apply the extension theorem to show that it would be possible to construct a product measure based on two marginal measures. 
                   \end{itemize}

}


\section{Foundation of Probability}

\subsection{Random Variable}

\subsubsection{Random Variable: Definition}
\frame
{
  \frametitle{Random Variable: Definition}

   \begin{itemize}

             \item<1->  On the probability triple $(\Omega, \mathcal{F},\BP)$, we define a random variable $X: \Omega \to \mathbf{R}$ if 
             
             $\{\omega: X(\omega) \leq x \} \in \mathcal{F}$ for any $x\in \mathbf{R}$  or 
             
             $\{\omega: X^{-1}(B) \} \in \mathcal{F}$ for any $B\in \mathcal{B}$.
             
\item<2->  The second condition certainly implies the first condition. To see why the first condition implies the second condition, define  $\mathcal{A}=\{A \subseteq \mathbf{R} : X^{-1}(A)\in \mathcal{F}\}$. $\mathcal{A}$ is a $\sigma$-algebra. Moreover, for any $x \in \mathbf{R}$, $(-\infty, x] \in \mathcal{A}$. Last, by definition of the Borel $\sigma$-algebra, $ \mathcal{B} \subseteq \mathcal{A}$. So, for any  $B\in \mathcal{B}$,   $X^{-1}(B)  \in \mathcal{F}$.      
                          
\item<3->  $X$ is a ``measurable" function from  $\Omega$ to $\mathbf{R}$. Generally speaking, we say a function from $(\Omega_1, \mathcal{F}_1,\BP_1)$ to  $(\Omega_2, \mathcal{F}_2,\BP_2)$ is measurable if the inverse image of any measurable set in $\Omega_2$ is also measurable in $\Omega_1$. 

\end{itemize}
}


\subsubsection{Random Variable: Property}


\frame
{
  \frametitle{Proposition 3.1.5}

   \begin{itemize}

\item<1->  The indicator of a measurable set $A$ (written $1_A(\omega)$) is a random variable. Take any $B \in \mathcal{B}$. $1$ can be in $B$ and/or $0$ can be in $B$, or neither. $X^{-1}(B)$ is then either $A$, $A^c$, $\Omega$ or $\emptyset$, all of which are in $\mathcal{F}$.  

\item<2-> $X+c$ and $cX$ are random variables if $X$ is.

\item<3-> $X^2$ is an rv if $X$ is. $\{X^2 \le y\} = \{ - \sqrt{y} \le X \le \sqrt{y} \} \in \mathcal{F}$.
                                        
                 \end{itemize}
}

\frame
{
  \frametitle{Proposition 3.1.5}

   \begin{itemize}

    \item<1->  If $X$ and $Y$ are r.v.s., then $X+Y$ is still random variable as:
              
                  $\{X+Y\leq z\}=\bigcup_{r \in \mathbb{Q}} \{X\leq r\} \cap \{Y\leq z-r\} $
                 
    \item<3->  If $Z_1, Z_2, \ldots$ are r.v.s., and $\lim_{n\rightarrow \infty} Z_n(\omega)$ exists for every $\omega$ , then $Z=\lim_{n\rightarrow \infty} Z_n$ is also a random variable
                                  
\item<3->[-]  
\begin{align*}
\{Z\leq z\} &= \{ \limsup_{n} Z_n(\omega) \leq z\} \\
&= \{ \lim_{n\to\infty} \sup_{k \ge n} Z_k(\omega) \leq z\} \\
&= \cup_{n =1}^{\infty} \{  \sup_{k \ge n} Z_k(\omega) \leq z\} \\
&= \cup_{n =1}^{\infty} \cap_{k=n}^{\infty} \{  Z_k(\omega) \leq z\}
\end{align*}

% which is to say, if the limit of $Z_n$, $Z$, is smaller th  an $z$, then for any (first intersection) small number such as $1/m$, we can find a large number $N$ (first union), such that for all (second intersection) $n\geq N$, $Z_n\leq z+\frac{1}{m}$. 
%                   

\end{itemize}
}


\subsection{Independence}

\subsubsection{Independence: Definition}
\frame
{
  \frametitle{Independence: Definition}

   \begin{itemize}

             \item<1->  A collection of events $\{A_{\alpha}\}_{\alpha\in I}$ (can be finite, countable or uncountable) are independent if and only if for all finite subsets $(\alpha_1, \ldots, \alpha_J)\subseteq I$, $P(\bigcap_{j=1}^J A_{\alpha_j})=\prod_{j=1}^J P(A_{\alpha_j})$.
             
                     \item<2->[-] It is possible to have any two of the three events to be independent but the three events together are not. For instance, throw a fair dice twice, let $A=\{\text{First toss is even}\}$, $B=\{\text{Second toss is odd}\}$ and $C=\{\text{Sum of the first and second dices is even}\}$. 
           
              \item<3->  A collection of random variable $\{X_{\alpha}\}_{\alpha\in I}$ are independent if and only if for all finite subsets $(\alpha_1, \ldots, \alpha_J)\subseteq I$, and all Borel sets $S_1, \ldots, S_J$, $P(X_{\alpha_1}\in S_1, \ldots, X_{\alpha_J}\in S_J)= P(X_{\alpha_1}\in S_1)\ldots P( X_{\alpha_J}\in S_J) $. 
                                       
                                
              \item<4->  If random variable $X$ and $Y$ are independent, then $f(X)$ and $g(Y)$ are also independent for measurable functions $f$ and $g$. 
              
                           
                 \end{itemize}
}

\end{document}
